---
title: "A user's guide to standard covariate measurement error correction for R"
author: "Linda Nab"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{standard_covme}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
options(rmarkdown.html_vignette.check_title = FALSE)
library(mecor)
```
## Introduction
__mecor__ is an R package for Measurement Error CORrection. __mecor__ implements measurement error correction methods for linear models with continuous outcomes. The measurement error can either occur in a continuous covariate or in the continuous outcome. This vignette discusses covariate measurement error correction by means of *standard regression calibration* in __mecor__. 

*Regression calibration* is one of the most popular measurement error correction methods for covariate measurement error. This vignette shows how *standard regression calibration* is applied in an internal validation study, a replicates study, a calibration study and an external validation study. Each of the four studies will be introduced in the subsequent sections, along with examples of how *standard regression calibration* can be applied in each of the four studies using __mecor__'s function `mecor()`. In all four studies, our interest lies in estimating the association between a continuous reference exposure $X$ and a continuous outcome $Y$, given covariates $Z$. Instead of $X$, the substitute error-prone exposure $X^*$ is measured. 

## Internal validation study
The simulated data set `icvs` in __mecor__ is an internal covariate-validation study. An internal validation study includes a subset of individuals of whom the reference exposure $X$ is observed. The data set `icvs` contains 1000 observations of the outcome $Y$, the error-prone exposure $X^*$ and the covariate $Z$. The reference exposure $X$ is observed in approximately 25% of the individuals in the study.
```{r load_data, eval = TRUE}
# load internal covariate validation study
data("icvs", package = "mecor")
head(icvs)
```
When ignoring the measurement error in $X^*$, one would naively regress $X^*$ and $Z$ on $Y$. This results in a biased estimation of the exposure-outcome association:
```{r uncorfit, eval = TRUE}
# naive estimate of the exposure-outcome association
lm(Y ~ X_star + Z, data = icvs)
```
Alternatively, one could perform an analysis restricted to the internal validation set: 
```{r ivr, eval = TRUE}
# analysis restricted to the internal validation set
lm(Y ~ X + Z, data = subset(icvs, !is.na(X)))
```
Although the above would result in an unbiased estimation of the exposure-outcome association, approximately 75% of the data is thrown out. Instead of doing an analysis restricted to the internal validation set, you could use *standard regression calibration* to correct for the measurement error in $X^*$. The following code chunk shows *standard regression calibration* with `mecor()`:
```{r rc, eval = TRUE}
mecor(formula = Y ~ MeasError(substitute = X_star, reference = X) + Z,
      data = icvs,
      method = "standard", # defaults to "standard"
      B = 0) # defaults to 0
```
As shown in the above code chunk, the `mecor()` function needs a *formula* argument, a *data* argument, a *method* argument and a *B* argument. Presumably, you are familiar with the structure of a formula in R. The only thing that's different here is the use of a `MeasError()` object in the formula. A `MeasError()` object is used to declare the substitute measure, in our case $X^*$, and the reference measure, in our case $X$. The *B* argument of `mecor()` is used to calculate bootstrap confidence intervals for the corrected coefficients of the model. Let us construct 95% confidence intervals using the bootstrap with 999 replicates:
```{r rc_se, eval = TRUE, results = 'hide'}
# save corrected fit
rc_fit <- 
  mecor(formula = Y ~ MeasError(substitute = X_star, reference = X) + Z,
        data = icvs,
        method = "standard", # defaults to "standard"
        B = 999) # defaults to 0
```
Print the confidence intervals to the console using `summary()`:
```{r rc_se_sum, eval = TRUE}
summary(rc_fit)
```
Two types of 95% confidence intervals are shown in the output of the `summary()` object. Bootstrap confidence intervals and Delta method confidence intervals. The default method to constructing confidence intervals in __mecor__ is the Delta method. Further, Fieller method confidence intervals and zero variance method confidence intervals can be constructed with `summary()`:
```{r rc_se2, eval = TRUE}
# fieller method ci and zero variance method ci and se's for 'rc_fit'
summary(rc_fit, zerovar = TRUE, fieller = TRUE)
```
Fieller method confidence intervals are only constructed for the corrected covariate (in this case $X$). 

## Replicates study
The simulated data set `rs` in __mecor__ is a replicates study. A replicates study includes a subset of individuals of whom the error-prone substitute exposure is repeatedly measured. The dataset `rs` contains 1000 observations of the outcome Y, three replicate measures of the error-prone exposure $X_1^*, X_2^*$ and $X_3^*$, and two covariates $Z_1$ and $Z_2$. It is assumed that there is 'random' measurement error in the repeatedly measured substitute exposure measure. 
```{r load_data2, eval = TRUE}
# load replicates study
data("rs", package = "mecor")
head(rs)
```
When ignoring the measurement error in $X_1^*$, one would naively regress $X_1^*$, $Z_1$ and $Z_2$ on $Y$. Which results in a biased estimation of the exposure-outcome association:
```{r uncorfit2, eval = TRUE}
# naive estimate of the exposure-outcome association
lm(Y ~ X_star_1 + Z1 + Z2, 
   data = rs)
```
Or alternatively, one could calculate the mean of each of the three replicate measures. Yet, this would still lead to a biased estimation of the exposure-outcome association:
```{r uncorfit3, eval = TRUE}
## calculate the mean of the three replicate measures
rs$X_star_123 <- with(rs, rowMeans(cbind(X_star_1, X_star_2, X_star_3)))
# naive estimate of the exposure-outcome association version 2
lm(Y ~ X_star_123 + Z1 + Z2,
   data = rs)
```
For an unbiased estimation of the exposure-outcome association, one could use regression calibration using `mecor()`:
```{r rc2, eval = TRUE}
mecor(formula = Y ~ MeasError(X_star_1, replicate = cbind(X_star_2, X_star_3)) + Z1 + Z2,
      data = rs)
```
Instead of using the *reference* argument in the `MeasError()` object, the *replicate* argument is used. Standard errors of the regression calibration estimator and confidence intervals can be constructed similar to what was shown for an internal validation study.

## Calibration study
The simulated data set `ccs` in __mecor__ is a covariate calibration study. In a calibration study, two types of measurements are used to measure the exposure. A measurement method prone to 'systematic' error, and a measurement method prone to 'random' error. The measurement prone to 'systematic' error is observed in the full study, the measurement prone to 'classical' erorr is observed in a subset of the study and repeatedly measured. The dataset `ccs` contains 1000 observations of the outcome $Y$, the substitute exposure $X^{*^s}$ prone to systematic error, the covariate $Z$ and two replicates of the substitute exposure prone $X^*_1$ and $X^*_2$ prone to 'random' error. The two replicate measures are observed in the first 500 individuals of the study population.
```{r load_data3, eval = TRUE}
# load calibration study
data("ccs", package = "mecor")
head(ccs)
```
When ignoring the measurement error in $X^{*^s}$, one would naively regress $X^{*^s}$ and $Z$ on $Y$. Which results in a biased estimation of the exposure-outcome association:
```{r uncorfit4, eval = TRUE}
## uncorrected regression
lm(Y ~ X_star + Z, 
   data = ccs)
```
Alternatively, one could use the first half of the study population and use the mean of each of the two replicate measures. Yet, this would still lead to a biased estimation of the exposure-outcome association:
```{r uncorfit5, eval = TRUE}
## calculate mean of three replicate measures
ccs$X_star_12 <- with(ccs, rowMeans(cbind(X_star_1, X_star_2)))
## uncorrected regression version 2
lm(Y ~ X_star_12 + Z,
   data = ccs)
```
For an unbiased estimation of the exposure-outcome association, one could use *standard regression calibration* using `mecor()`:
```{r rc3, eval = TRUE}
mecor(formula = Y ~ MeasError(X_star, replicate = cbind(X_star_1, X_star_2)) + Z,
      data = ccs)
```
The reader who is paying attention would perhaps notice that the subset of 500 individuals of whom $X_1^*$ and $X_2^*$ were observed resembles a replicates study. *Standard regression calibration* therefore could alternatively be applied in the following way:
```{r rc4, eval = TRUE}
mecor(formula = Y ~ MeasError(X_star_1, replicate = X_star_2) + Z,
      data = subset(ccs, !is.na(X_star_2)))
```
Standard errors of the regression calibration estimator and confidence intervals can be constructed similar to what was shown for an internal validation study.

## External validation study
The simulated data set `ecvs` in __mecor__ is a external covariate-validation study. An external validation study is used when in the main study, no information is available to correct for the measurement error in the exposure $X$. Suppose for example that  $X$ is not observed in the internal covariate-validation study `icvs`. An external validation study is a (small) sub study containing observations of the reference measure $X$, the error-prone substitute measure $X^*$ and the covariate $Z$ of the original study. The external validation study is then used to estimate the calibration model, that is subsequently used to correct for the measurement error in the main study. 
```{r load_data4, eval = TRUE}
# load internal covariate validation study
data("ecvs", package = "mecor")
head(ecvs)
data("icvs", package = "mecor")
```
Suppose reference measure $X$ is not observed in dataset `icvs`. To correct the bias in the naive association between exposure $X^*$ and outcome $Y$ given $Z$, using the external validation study, one can proceed as follows using `mecor()`:
```{r extrc, eval = TRUE}
# Estimate the calibration model in the external validation study
calmod <- lm(X ~ X_star + Z, 
             data = ecvs)
# Use the calibration model for measurement error correction:
mecor(Y ~ MeasErrorExt(X_star, model = calmod) + Z,
      data = icvs)
```
Here, a `MeasErrorExt()` object is used, since external information is used for measurement error correction. The model argument takes a linear model of class lm or a named list with the coefficients of the calibration model:
```{r extrc2, eval = TRUE}
# Use coefficiets for measurement error correction:
mecor(Y ~ MeasErrorExt(X_star, model = list(coef = c(0, 0.8, 2))) + Z,
      data = icvs)
```



